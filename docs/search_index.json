[["index.html", "A Quick Start Guide to Survey Research Welcome to survey research", " A Quick Start Guide to Survey Research Liz Carey (and hopefully many others) 2021-12-30 Welcome to survey research This book is intended to be a quick resource for conducting survey research. By no means is it intended to be comprehensive of all survey research methodologies. "],["preface.html", "Preface Outline Prerequisites Acknowledgements", " Preface Hopefully you’ll find this book to be a condensed and easy to read resource on survey research. I developed this book in the hopes of future collaboration among other UX researchers. Outline Inside this book you’ll find resources on: Chapter 1: Designing a survey Chapter 2: Writing effective survey questions Chapter 3: Survey Analysis Prerequisites All you need is an interest in conducting survey research and basic data analysis. I’ll include snippets of R code to perform the analysis along the way, but if you want a more in-depth primer on R, feel free to check out this Intro to R workshop I’ve adapted from Amelia McNamara’s R Workshop Acknowledgements This book wouldn’t be possible without the contributions of: TBD "],["macro.html", "Chapter 1 Designing a survey 1.1 Choosing a research method 1.2 Selecting your target population 1.3 Total survey error framework", " Chapter 1 Designing a survey 1.1 Choosing a research method First, establish if a survey is the right method to accomplish your research goal. Below are two useful visualizations from the Nielsen Norman group on how to decide between which qualitative or quantitative methods to answer your research goal (Rohrer 2014). Surveys are great for answering the “How many and how much” of what people do and say; surveys are not the best method at understanding the “Why and how to fix” a product problem. 1.2 Selecting your target population Once you’ve determined your research goal, it’s important to determine your target audience. In an ideal world, we’d sample the entire population of interest. But because of resource constraints (i.e. it can be expensive to run a survey, we don’t want to exhaust our population/prevent them from taking future surveys), we have to be deliberate in who we select into our study. A common question asked is: “How many people do I need to survey?” The answer to this question is entirely dependent on the level of power and effect size your team is targeting. When determining you survey’s sample size, your goal should be to collect a large enough sample to have sufficient power to detect a meaningful effect. 1.3 Total survey error framework There are two types of errors that commonly arise in surveys. Measurement error: how well do our survey questions measure the constructs of interest (e.g. construct validity, measurement error, processing error) Representation error: how much does our survey statistic generalize to our target population (e.g. coverage error, sampling error, nonresponse error, adjustment error) References "],["writing-effective-survey-questions.html", "Chapter 2 Writing effective survey questions 2.1 Cognitive Response Model", " Chapter 2 Writing effective survey questions 2.1 Cognitive Response Model Before drafting your questions, it’s helpful to understand how respondents think when answering a survey. The following is a 4-stage cognitive model respondents when they complete a survey. 1. Comprehension 2. Retrieval of information 3. Judgment or estimation 4. Selection of a response to the question 2.1.1 Comprehension Comprehension is understanding the task and question. The following are tips on how to improve survey comprehension of respondents (Jon A. Krosnick 2010): Use simple, familiar words (avoid technical terms, jargon, and slang) Use simple syntax Avoid words with ambiguous meanings, i.e., aim for wording that all respondents will interpret in the same way Strive for wording that is specific and concrete (as opposed to general and abstract) Make response options exhaustive and mutually exclusive Avoid leading or loaded questions that push respondents toward an answer Ask about one thing at a time (avoid double-barreled questions) and Avoid questions with single or double negations How to optimize question ordering (Jon A. Krosnick 2010): Early questions should be easy and pleasant to answer. Questions at the very beginning of a questionnaire should explicitly address the topic of the survey. Questions on the same topic should be grouped together. Questions on the same topic should proceed from general to specific. Questions on sensitive topics that might make respondents uncomfortable should be placed at the end of the questionnaire. Only ask questions that are relevant to the respondents: filter/skip questions that do not apply. 2.1.2 Retrieval of information Recalling information from memory, bringing information into consciousness, explicit memory 2.1.3 Judgment or estimation Combining, supplementing, extrapolating from information that has been retrieved. 2.1.4 Selection of a response to the question References "],["analysis.html", "Chapter 3 Survey Analysis 3.1 Organize your workspace 3.2 Data Cleaning", " Chapter 3 Survey Analysis After you’ve fielded your survey, here are the steps to making sense of the data. This section assumes you have a laptop set up to work with in either R or python. Head over to the Appendix page if you need help with set up. 3.1 Organize your workspace Before beginning any analysis, you’ll want to set up a reproducible workflow. Below is an adapted suggestion on how to organize your workspace from Ben Marwick, Carl Boettiger, and Lincoln Mullen (Ben Marwick 2018). Keeping your workspace organized is the best way for you and others to understand and reproduce your analysis. project |- DESCRIPTION # project metadata and dependencies |- README.md # top-level description of content and guide to users | |- data/ # data files used | +- raw_data.csv # data files in open formats such as TXT, CSV, TSV, etc. | +- cleaned_data.csv # data files that have been cleaned, merged, etc that you&#39;ll use for survey analysis | |- analysis/ # any programmatic code | +- my_report.Rmd # R markdown file with narrative text interwoven with code chunks | +- makefile # builds a PDF/HTML/DOCX file from the Rmd, code, and data files | +- scripts/ # code files (R, shell, etc.) used for data cleaning, analysis and visualisation | +- figures/ # saved outputs of your figures | |- R/ | +- my_functions.R # custom R functions that are used more than once throughout the project | |- man/ | +- my_functions.Rd # documentation for the R functions (auto-generated when using devtools) | R version #List the directory names you want to create folder_names &lt;- c(&quot;data&quot;, &quot;data/raw&quot;, &quot;data/clean&quot;, &quot;analysis&quot;, &quot;analysis/scripts&quot;, &quot;analysis/figures&quot;, &quot;R&quot;) #Create the directories sapply(folder_names, dir.create) 3.2 Data Cleaning Before you can begin looking at the results, you’ll need to clean the data. By “cleaning” the data, we mean edited the raw file into a format that will make the analysis valid and easier. 3.2.1 Load the data Download your raw survey data as a csv and load it into your your analysis tool of choice (e.g. Ipython notebook or Rstudio) R version # load necessary packages for analysis library(tidyverse) #contains all the library packages to manipulate and transform data library(summarytools) #shortcut tools to visualize summaries of the data # read/store the data as the variable df (short for dataframe) # replace &quot;file&quot; with &quot;https://raw.githubusercontent.com/lizmcarey/survey-guide/master/sample_data/Survey_test_data.csv&quot; to download the survey data set file &lt;- &quot;./sample_data/Survey_test_data.csv&quot; #load file from folder heirarchy df &lt;- read_csv(file) python version #load necessary modules for analysis import pandas as pd #read/store the data as the variable df (short for dataframe) df = pd.read_csv(filename) 3.2.2 Loading Qualtrics data When you download a csv from Qualtrics, it will come with a few extra rows you don’t need. Here are some automated scripts you can add to your makefile to speed up your workflow R version manual # Store the column names by reading in the column header df_names &lt;- read_csv(file, n_max=0) %&gt;% names() # Read the entire file df &lt;- read_csv(file, col_names = df_names, # use df_names to title the columns skip = 3) # skip the first three lines #store the question names question_bank &lt;- read_csv(file, n_max=1) %&gt;% # read in the first row of the file select(starts_with(&quot;Q&quot;)) %&gt;% # select columns that start with Q gather(key, question_text) # transform data from wide to long R version programmatic #function to load qualtrics csv and remove extra rows load_qualtrics_csv &lt;- function(file) { df_names &lt;- read_csv(file, n_max = 0) %&gt;% names() df &lt;- read_csv(file, col_names = df_names, skip = 3) } #function to store questions get_questions &lt;- function(file) { qb &lt;- read_csv(file, n_max = 1) %&gt;% select(starts_with(&quot;Q&quot;)) %&gt;% gather(key, question_text) } #Use function to read in survey file, and skip first 3 lines df &lt;- load_qualtrics_csv(file) #Use function to store question wording question_bank &lt;- get_questions(file) 3.2.3 Preview the data It’s important to get a look at the data to spot any errors in uploading the dataset and the validity of the responses. You’ll want to check for: Total number of observations/rows Duplicate responses Drop off/Abandon rate of the survey Average survey completion time “Speeders:” those who couldn’t have completed the survey in a reasonable amount of time There are multiple different ways to preview your dataset before analysis. One quick way is to check the first few rows of your data. You can do this with the function head(). #Check the first 5 rows of data head(df) ## # A tibble: 6 × 29 ## StartDate EndDate Status IPAddress Progress ## &lt;dttm&gt; &lt;dttm&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 2019-01-15 13:28:39 2019-01-15 13:28:39 Survey Test NA 100 ## 2 2019-01-15 13:28:40 2019-01-15 13:28:40 Survey Test NA 100 ## 3 2019-01-15 13:36:47 2019-01-15 13:36:47 Survey Test NA 100 ## 4 2019-01-15 13:36:47 2019-01-15 13:36:47 Survey Test NA 100 ## 5 2019-01-15 13:36:48 2019-01-15 13:36:48 Survey Test NA 100 ## 6 2019-01-15 13:36:48 2019-01-15 13:36:48 Survey Test NA 100 ## # … with 24 more variables: Duration (in seconds) &lt;dbl&gt;, Finished &lt;lgl&gt;, ## # RecordedDate &lt;dttm&gt;, ResponseId &lt;chr&gt;, RecipientLastName &lt;lgl&gt;, ## # RecipientFirstName &lt;lgl&gt;, RecipientEmail &lt;lgl&gt;, ExternalReference &lt;lgl&gt;, ## # LocationLatitude &lt;dbl&gt;, LocationLongitude &lt;dbl&gt;, DistributionChannel &lt;chr&gt;, ## # UserLanguage &lt;lgl&gt;, Q1 &lt;chr&gt;, Q2 &lt;chr&gt;, Q3_4 &lt;chr&gt;, Q3_5 &lt;chr&gt;, Q3_6 &lt;chr&gt;, ## # Q3_7 &lt;chr&gt;, Q3_8 &lt;chr&gt;, Q3_9 &lt;chr&gt;, Q3_10 &lt;chr&gt;, Q3_10_TEXT &lt;chr&gt;, Q4 &lt;chr&gt;, ## # Q5 &lt;chr&gt; A more comprehensive way to view your dataset is with the skimr package. This package will give an overview of the number of observations and variables in your data. The missing column should not be greater than 20% of your total number of observations (unless it’s a multiselect question). Questions with dropoff greater than 20% can signal that the question was difficult for respondents to answer; you should be wary of response bias and consider removing the question from analysis and rewording the question for future survey sends. library(skimr) skim(df) Table 3.1: Data summary Name df Number of rows 502 Number of columns 29 _______________________ Column type frequency: character 15 logical 7 numeric 4 POSIXct 3 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace Status 0 1.00 11 11 0 1 0 ResponseId 0 1.00 17 17 0 502 0 DistributionChannel 0 1.00 4 4 0 1 0 Q1 0 1.00 5 14 0 6 0 Q2 0 1.00 18 34 0 5 0 Q3_4 201 0.60 26 26 0 1 0 Q3_5 165 0.67 22 22 0 1 0 Q3_6 174 0.65 21 21 0 1 0 Q3_7 172 0.66 19 19 0 1 0 Q3_8 184 0.63 18 18 0 1 0 Q3_9 162 0.68 23 23 0 1 0 Q3_10 184 0.63 5 5 0 1 0 Q3_10_TEXT 184 0.63 51 135 0 318 0 Q4 0 1.00 11 22 0 7 0 Q5 0 1.00 53 134 0 502 0 Variable type: logical skim_variable n_missing complete_rate mean count IPAddress 502 0 NaN : Finished 0 1 1 TRU: 502 RecipientLastName 502 0 NaN : RecipientFirstName 502 0 NaN : RecipientEmail 502 0 NaN : ExternalReference 502 0 NaN : UserLanguage 502 0 NaN : Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Progress 0 1 100.00 0.00 100.00 100.00 100.00 100.00 100.00 ▁▁▇▁▁ Duration (in seconds) 0 1 0.02 0.15 0.00 0.00 0.00 0.00 1.00 ▇▁▁▁▁ LocationLatitude 0 1 37.77 0.00 37.77 37.77 37.77 37.77 37.77 ▁▁▇▁▁ LocationLongitude 0 1 -122.41 0.00 -122.41 -122.41 -122.41 -122.41 -122.41 ▁▁▇▁▁ Variable type: POSIXct skim_variable n_missing complete_rate min max median n_unique StartDate 0 1 2019-01-15 13:28:39 2019-01-15 13:37:58 2019-01-15 13:37:22 74 EndDate 0 1 2019-01-15 13:28:39 2019-01-15 13:37:58 2019-01-15 13:37:22 74 RecordedDate 0 1 2019-01-15 13:28:39 2019-01-15 13:37:58 2019-01-15 13:37:22 74 Another package that can give a brief overview of your data is summarytools library(summarytools) view(dfSummary(df)) # use view lowercase to see html output in the Rstudio viewer pane 3.2.4 Joining data sets Sometimes the data you need lives in two tables. dplyr from the tidyverse package makes it easy to join your data sets together. In order to join two tables together, you’ll need a shared unique identifier across the two tables. Below are all the ways you can join two data sets using R and the corresponding dplyr functions. You can view this image and additional ways to transform data sets on the RStudio Data Wrangling Cheat Sheet. In Appendix C, I’ve generated a fake dataset with corresponding ResponseId’s that match to the survey data set (df). Below I use a left join to merge respondent data table with the survey data table. df &lt;- df %&gt;% left_join(respondent_data, by = c(&quot;ResponseId&quot; = &quot;ResponseId&quot;)) # View merged data sets skim(df) Table 3.2: Data summary Name df Number of rows 502 Number of columns 36 _______________________ Column type frequency: character 22 logical 7 numeric 4 POSIXct 3 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace Status 0 1.00 11 11 0 1 0 ResponseId 0 1.00 17 17 0 502 0 DistributionChannel 0 1.00 4 4 0 1 0 Q1 0 1.00 5 14 0 6 0 Q2 0 1.00 18 34 0 5 0 Q3_4 201 0.60 26 26 0 1 0 Q3_5 165 0.67 22 22 0 1 0 Q3_6 174 0.65 21 21 0 1 0 Q3_7 172 0.66 19 19 0 1 0 Q3_8 184 0.63 18 18 0 1 0 Q3_9 162 0.68 23 23 0 1 0 Q3_10 184 0.63 5 5 0 1 0 Q3_10_TEXT 184 0.63 51 135 0 318 0 Q4 0 1.00 11 22 0 7 0 Q5 0 1.00 53 134 0 502 0 name 0 1.00 8 30 0 502 0 first_name 0 1.00 2 11 0 388 0 job 0 1.00 4 59 0 346 0 phone_number 0 1.00 11 20 0 502 0 email 0 1.00 9 23 0 417 0 gender 0 1.00 4 17 0 4 0 age 0 1.00 3 8 0 4 0 Variable type: logical skim_variable n_missing complete_rate mean count IPAddress 502 0 NaN : Finished 0 1 1 TRU: 502 RecipientLastName 502 0 NaN : RecipientFirstName 502 0 NaN : RecipientEmail 502 0 NaN : ExternalReference 502 0 NaN : UserLanguage 502 0 NaN : Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Progress 0 1 100.00 0.00 100.00 100.00 100.00 100.00 100.00 ▁▁▇▁▁ Duration (in seconds) 0 1 0.02 0.15 0.00 0.00 0.00 0.00 1.00 ▇▁▁▁▁ LocationLatitude 0 1 37.77 0.00 37.77 37.77 37.77 37.77 37.77 ▁▁▇▁▁ LocationLongitude 0 1 -122.41 0.00 -122.41 -122.41 -122.41 -122.41 -122.41 ▁▁▇▁▁ Variable type: POSIXct skim_variable n_missing complete_rate min max median n_unique StartDate 0 1 2019-01-15 13:28:39 2019-01-15 13:37:58 2019-01-15 13:37:22 74 EndDate 0 1 2019-01-15 13:28:39 2019-01-15 13:37:58 2019-01-15 13:37:22 74 RecordedDate 0 1 2019-01-15 13:28:39 2019-01-15 13:37:58 2019-01-15 13:37:22 74 3.2.5 Removing duplicate values Respondents may come back to the survey, or try to take the survey a second time on a new device. To ensure a respondent isn’t counted more than once in a survey, be sure to check for duplicate values by using a unique identifier. Common unique indentifiers include: email, embedded user id, or IP address. View duplicates using janitor package library(janitor) df %&gt;% get_dupes(email) # get_dupes is a function available through janitor, can use more than one column to view duplicates ## # A tibble: 108 × 37 ## email dupe_count StartDate EndDate Status IPAddress ## &lt;glue&gt; &lt;int&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 dr@gmail… 7 2019-01-15 13:36:48 2019-01-15 13:36:48 Survey… NA ## 2 dr@gmail… 7 2019-01-15 13:36:56 2019-01-15 13:36:56 Survey… NA ## 3 dr@gmail… 7 2019-01-15 13:37:19 2019-01-15 13:37:19 Survey… NA ## 4 dr@gmail… 7 2019-01-15 13:37:20 2019-01-15 13:37:20 Survey… NA ## 5 dr@gmail… 7 2019-01-15 13:37:25 2019-01-15 13:37:25 Survey… NA ## 6 dr@gmail… 7 2019-01-15 13:37:29 2019-01-15 13:37:29 Survey… NA ## 7 dr@gmail… 7 2019-01-15 13:37:48 2019-01-15 13:37:48 Survey… NA ## 8 dr@hotma… 4 2019-01-15 13:36:57 2019-01-15 13:36:57 Survey… NA ## 9 dr@hotma… 4 2019-01-15 13:37:10 2019-01-15 13:37:10 Survey… NA ## 10 dr@hotma… 4 2019-01-15 13:37:23 2019-01-15 13:37:23 Survey… NA ## # … with 98 more rows, and 31 more variables: Progress &lt;dbl&gt;, ## # Duration (in seconds) &lt;dbl&gt;, Finished &lt;lgl&gt;, RecordedDate &lt;dttm&gt;, ## # ResponseId &lt;chr&gt;, RecipientLastName &lt;lgl&gt;, RecipientFirstName &lt;lgl&gt;, ## # RecipientEmail &lt;lgl&gt;, ExternalReference &lt;lgl&gt;, LocationLatitude &lt;dbl&gt;, ## # LocationLongitude &lt;dbl&gt;, DistributionChannel &lt;chr&gt;, UserLanguage &lt;lgl&gt;, ## # Q1 &lt;chr&gt;, Q2 &lt;chr&gt;, Q3_4 &lt;chr&gt;, Q3_5 &lt;chr&gt;, Q3_6 &lt;chr&gt;, Q3_7 &lt;chr&gt;, ## # Q3_8 &lt;chr&gt;, Q3_9 &lt;chr&gt;, Q3_10 &lt;chr&gt;, Q3_10_TEXT &lt;chr&gt;, Q4 &lt;chr&gt;, Q5 &lt;chr&gt;, … Manual way to view duplicates u_id &lt;- quo(email) # Store unique identifier column, can be IP address, email, etc. df %&gt;% group_by(!!u_id) %&gt;% tally() %&gt;% filter(n &gt; 1) ## # A tibble: 23 × 2 ## email n ## &lt;glue&gt; &lt;int&gt; ## 1 dr@gmail.com 7 ## 2 dr@hotmail.com 4 ## 3 dr@me.com 7 ## 4 dr@outlook.com 7 ## 5 dr@yahoo.com 8 ## 6 ken@hotmail.com 2 ## 7 miss@gmail.com 3 ## 8 miss@hotmail.com 2 ## 9 miss@me.com 5 ## 10 miss@outlook.com 4 ## # … with 13 more rows You’ll want to remove duplicate responses, and keep the most recent response. library(lubridate) # load library for converting datetimes #Remove duplicate emails, keep most recent submission df &lt;- df %&gt;% mutate(EndDate = as_datetime(EndDate, tz = &quot;America/Los_Angeles&quot;)) %&gt;% # converts column to a datetime format filter(!is.na(!!u_id)) %&gt;% group_by(!!u_id) %&gt;% slice(which.max(EndDate)) %&gt;% ungroup() References "],["appendixA.html", "A Setting up R A.1 Package installation", " A Setting up R A.1 Package installation You’ll want to install the following packages: library(tidyverse) "],["appendixB.html", "B Setting up python", " B Setting up python # Pandas makes working with data tables easier import pandas as pd # Numpy is a library for working with Arrays import numpy as np # Module for plotting graphs import matplotlib.pyplot as plt import seaborn as sns # SciPy implements many different numerical algorithms import scipy.stats as stats import collections "],["appendixC.html", "C Generating fake data", " C Generating fake data Here’s the code I used to create the respondent information table library(charlatan) # library of fake data library(glue) # library for pasting together variables email_domains &lt;- c(&quot;@gmail.com&quot;, &quot;@hotmail.com&quot;, &quot;@outlook.com&quot;, &quot;@me.com&quot;, &quot;@yahoo.com&quot;) respondent_data &lt;- ch_generate(&#39;name&#39;, &#39;job&#39;, &#39;phone_number&#39;, n = nrow(df)) %&gt;% separate(name, &quot;first_name&quot;, extra = &quot;drop&quot;, remove=FALSE) %&gt;% mutate(email = glue(&quot;{first_lower}{email_domain}&quot;, first_lower = tolower(first_name), email_domain = sample(email_domains, nrow(df), replace=TRUE) ), gender = sample(c(&quot;male&quot;, &quot;female&quot;, &quot;other&quot;, &quot;prefer not to say&quot;), nrow(df), replace=TRUE, prob=c(0.55, 0.35, 0.05, 0.05)), age = sample(c(&quot;Under 18&quot;, &quot;18-34&quot;, &quot;35-54&quot;, &quot;55+&quot;), nrow(df), replace=TRUE, prob = c(0.05, 0.25, 0.40, 0.30)) ) # add ResponseId column from survey sample respondent_data &lt;- df %&gt;% select(ResponseId) %&gt;% bind_cols(respondent_data) write_csv(respondent_data, &quot;./sample_data/respondent_data.csv&quot;) # Store data in sample_data folder skim(respondent_data) Table C.1: Data summary Name respondent_data Number of rows 502 Number of columns 8 _______________________ Column type frequency: character 8 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace ResponseId 0 1 17 17 0 502 0 name 0 1 8 30 0 502 0 first_name 0 1 2 11 0 388 0 job 0 1 4 59 0 346 0 phone_number 0 1 11 20 0 502 0 email 0 1 9 23 0 417 0 gender 0 1 4 17 0 4 0 age 0 1 3 8 0 4 0 "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
