[
["index.html", "A Quick Start Guide to Survey Research Welcome to survey research", " A Quick Start Guide to Survey Research Liz Carey (and hopefully many others) 2019-01-26 Welcome to survey research This book is intended to be a quick resource for conducting survey research. By no means is it intended to be comprehensive of all survey research methodologies. "],
["preface.html", "Preface Outline Prerequisites Acknowledgements", " Preface Hopefully you’ll find this book to be a condensed and easy to read resource on survey research. We developed this book in the hopes of future collaboration among other UX researchers. Outline The content of the book will include: Chapter 1 Chapter 2 Prerequisites All you need is an interest in conducting survey research and basic data analysis, we’ll include code snippets (python and R) along the way. Acknowledgements This book wouldn’t be possible without the contributions of: "],
["macro.html", "Chapter 1 Designing a survey 1.1 What is your research goal? 1.2 Who are you studying?", " Chapter 1 Designing a survey 1.1 What is your research goal? First, establish if a survey is the right method to accomplish your research goal by asking yourself: What do you currently know? What don’t you know? Below is a useful visualization from the Nielsen Norman group on how to decide between which qualitative or quantitative methods to answer your research goal (Rohrer 2014). Surveys are great for answering the “How many and how much” of what people do and say; surveys are not the best method at understanding the “Why and how to fix” a product problem. 1.2 Who are you studying? This question may be simple at first, but when you start to narrow down References "],
["writing-effective-survey-questions.html", "Chapter 2 Writing effective survey questions", " Chapter 2 Writing effective survey questions Effective survey questions result in consistent and reliable responses. Surveys are NOT a shortcut for usability tests "],
["analysis.html", "Chapter 3 Survey Analysis 3.1 Organize your workspace 3.2 Data Cleaning", " Chapter 3 Survey Analysis After you’ve fielded your survey, here are the steps to making sense of the data. This section assumes you have a laptop set up to work with in either R or python. Head over to the Appendix page if you need help with set up. 3.1 Organize your workspace Before beginning any analysis, you’ll want to set up a reproducible workflow. Below is an adapted suggestion on how to organize your workspace from Ben Marwick, Carl Boettiger, and Lincoln Mullen (Ben Marwick 2018). Keeping your workspace organized is the best way for you and others to understand and reproduce your analysis. project |- DESCRIPTION # project metadata and dependencies |- README.md # top-level description of content and guide to users | |- data/ # data files used | +- raw_data.csv # data files in open formats such as TXT, CSV, TSV, etc. | +- cleaned_data.csv # data files that have been cleaned, merged, etc that you&#39;ll use for survey analysis | |- analysis/ # any programmatic code | +- my_report.Rmd # R markdown file with narrative text interwoven with code chunks | +- makefile # builds a PDF/HTML/DOCX file from the Rmd, code, and data files | +- scripts/ # code files (R, shell, etc.) used for data cleaning, analysis and visualisation | +- figures/ # saved outputs of your figures | |- R/ | +- my_functions.R # custom R functions that are used more than once throughout the project | |- man/ | +- my_functions.Rd # documentation for the R functions (auto-generated when using devtools) | R version #List the directory names you want to create folder_names &lt;- c(&quot;data&quot;, &quot;data/raw&quot;, &quot;data/clean&quot;, &quot;analysis&quot;, &quot;analysis/scripts&quot;, &quot;analysis/figures&quot;, &quot;R&quot;) #Create the directories sapply(folder_names, dir.create) 3.2 Data Cleaning Before you can begin looking at the results, you’ll need to clean the data. By “cleaning” the data, we mean edited the raw file into a format that will make the analysis valid and easier. 3.2.1 Load the data Download your raw survey data as a csv and load it into your your analysis tool of choice (e.g. Ipython notebook or Rstudio) R version # load necessary packages for analysis library(tidyverse) #contains all the library packages to manipulate and transform data library(summarytools) #shortcut tools to visualize summaries of the data # read/store the data as the variable df (short for dataframe) # replace &quot;file&quot; with &quot;https://raw.githubusercontent.com/lizmcarey/survey-guide/master/sample_data/Survey_test_data.csv&quot; to download the survey data set file &lt;- &quot;./sample_data/Survey_test_data.csv&quot; #load file from folder heirarchy df &lt;- read_csv(file) python version #load necessary modules for analysis import pandas as pd #read/store the data as the variable df (short for dataframe) df = pd.read_csv(filename) 3.2.2 Loading Qualtrics data When you download a csv from Qualtrics, it will come with a few extra rows you don’t need. Here are some automated scripts you can add to your makefile to speed up your workflow R version manual # Store the column names by reading in the column header df_names &lt;- read_csv(file, n_max=0) %&gt;% names() # Read the entire file df &lt;- read_csv(file, col_names = df_names, # use df_names to title the columns skip = 3) # skip the first three lines #store the question names question_bank &lt;- read_csv(file, n_max=1) %&gt;% # read in the first row of the file select(starts_with(&quot;Q&quot;)) %&gt;% # select columns that start with Q gather(key, question_text) # transform data from wide to long R version programmatic #function to load qualtrics csv and remove extra rows load_qualtrics_csv &lt;- function(file) { df_names &lt;- read_csv(file, n_max = 0) %&gt;% names() df &lt;- read_csv(file, col_names = df_names, skip = 3) } #function to store questions get_questions &lt;- function(file) { qb &lt;- read_csv(file, n_max = 1) %&gt;% select(starts_with(&quot;Q&quot;)) %&gt;% gather(key, question_text) } #Use function to read in survey file, and skip first 3 lines df &lt;- load_qualtrics_csv(file) #Use function to store question wording question_bank &lt;- get_questions(file) 3.2.3 Preview the data It’s important to get a look at the data to spot any errors in uploading the dataset and the validity of the responses. You’ll want to check for: Total number of observations/rows Duplicate responses Drop off/Abandon rate of the survey Average survey completion time “Speeders:” those who couldn’t have completed the survey in a reasonable amount of time There are multiple different ways to preview your dataset before analysis. One quick way is to check the first few rows of your data. You can do this with the function head(). #Check the first 5 rows of data head(df) ## # A tibble: 6 x 29 ## StartDate EndDate Status IPAddress Progress ## &lt;dttm&gt; &lt;dttm&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 2019-01-15 13:28:39 2019-01-15 13:28:39 Surve… NA 100 ## 2 2019-01-15 13:28:40 2019-01-15 13:28:40 Surve… NA 100 ## 3 2019-01-15 13:36:47 2019-01-15 13:36:47 Surve… NA 100 ## 4 2019-01-15 13:36:47 2019-01-15 13:36:47 Surve… NA 100 ## 5 2019-01-15 13:36:48 2019-01-15 13:36:48 Surve… NA 100 ## 6 2019-01-15 13:36:48 2019-01-15 13:36:48 Surve… NA 100 ## # … with 24 more variables: `Duration (in seconds)` &lt;dbl&gt;, Finished &lt;lgl&gt;, ## # RecordedDate &lt;dttm&gt;, ResponseId &lt;chr&gt;, RecipientLastName &lt;lgl&gt;, ## # RecipientFirstName &lt;lgl&gt;, RecipientEmail &lt;lgl&gt;, ## # ExternalReference &lt;lgl&gt;, LocationLatitude &lt;dbl&gt;, ## # LocationLongitude &lt;dbl&gt;, DistributionChannel &lt;chr&gt;, ## # UserLanguage &lt;lgl&gt;, Q1 &lt;chr&gt;, Q2 &lt;chr&gt;, Q3_4 &lt;chr&gt;, Q3_5 &lt;chr&gt;, ## # Q3_6 &lt;chr&gt;, Q3_7 &lt;chr&gt;, Q3_8 &lt;chr&gt;, Q3_9 &lt;chr&gt;, Q3_10 &lt;chr&gt;, ## # Q3_10_TEXT &lt;chr&gt;, Q4 &lt;chr&gt;, Q5 &lt;chr&gt; A more comprehensive way to view your dataset is with the skimr package. This package will give an overview of the number of observations and variables in your data. The missing column should not be greater than 20% of your total number of observations (unless it’s a multiselect question). Questions with dropoff greater than 20% can signal that the question was difficult for respondents to answer; you should be wary of response bias and consider removing the question from analysis and rewording the question for future survey sends. library(skimr) skim(df) ## Skim summary statistics ## n obs: 502 ## n variables: 29 ## ## ── Variable type:character ──────────────────────────────────────────────── ## variable missing complete n min max empty n_unique ## DistributionChannel 0 502 502 4 4 0 1 ## Q1 0 502 502 5 14 0 6 ## Q2 0 502 502 18 34 0 5 ## Q3_10 184 318 502 5 5 0 1 ## Q3_10_TEXT 184 318 502 51 135 0 318 ## Q3_4 201 301 502 26 26 0 1 ## Q3_5 165 337 502 22 22 0 1 ## Q3_6 174 328 502 21 21 0 1 ## Q3_7 172 330 502 19 19 0 1 ## Q3_8 184 318 502 18 18 0 1 ## Q3_9 162 340 502 23 23 0 1 ## Q4 0 502 502 11 22 0 7 ## Q5 0 502 502 53 134 0 502 ## ResponseId 0 502 502 17 17 0 502 ## Status 0 502 502 11 11 0 1 ## ## ── Variable type:logical ────────────────────────────────────────────────── ## variable missing complete n mean count ## ExternalReference 502 0 502 NaN 502 ## Finished 0 502 502 1 TRU: 502, NA: 0 ## IPAddress 502 0 502 NaN 502 ## RecipientEmail 502 0 502 NaN 502 ## RecipientFirstName 502 0 502 NaN 502 ## RecipientLastName 502 0 502 NaN 502 ## UserLanguage 502 0 502 NaN 502 ## ## ── Variable type:numeric ────────────────────────────────────────────────── ## variable missing complete n mean sd p0 p25 ## Duration (in seconds) 0 502 502 0.024 0.15 0 0 ## LocationLatitude 0 502 502 37.77 0 37.77 37.77 ## LocationLongitude 0 502 502 -122.41 0 -122.41 -122.41 ## Progress 0 502 502 100 0 100 100 ## p50 p75 p100 hist ## 0 0 1 ▇▁▁▁▁▁▁▁ ## 37.77 37.77 37.77 ▁▁▁▇▁▁▁▁ ## -122.41 -122.41 -122.41 ▁▁▁▇▁▁▁▁ ## 100 100 100 ▁▁▁▇▁▁▁▁ ## ## ── Variable type:POSIXct ────────────────────────────────────────────────── ## variable missing complete n min max median ## EndDate 0 502 502 2019-01-15 2019-01-15 2019-01-15 ## RecordedDate 0 502 502 2019-01-15 2019-01-15 2019-01-15 ## StartDate 0 502 502 2019-01-15 2019-01-15 2019-01-15 ## n_unique ## 74 ## 74 ## 74 Another package that can give a brief overview of your data is summarytools library(summarytools) view(dfSummary(df)) # use view lowercase to see html output in the Rstudio viewer pane 3.2.4 Joining data sets Sometimes the data you need lives in two tables. dplyr from the tidyverse package makes it easy to join your data sets together. In order to join two tables together, you’ll need a shared unique identifier across the two tables. Below are all the ways you can join two data sets using R and the corresponding dplyr functions. You can view this image and additional ways to transform data sets on the RStudio Data Wrangling Cheat Sheet. In Appendix C, I’ve generated a fake dataset with corresponding ResponseId’s that match to the survey data set (df). Below I use a left join to merge respondent data table with the survey data table. df &lt;- df %&gt;% left_join(respondent_data, by = c(&quot;ResponseId&quot; = &quot;ResponseId&quot;)) # View merged data sets skim(df) ## Skim summary statistics ## n obs: 502 ## n variables: 36 ## ## ── Variable type:character ──────────────────────────────────────────────── ## variable missing complete n min max empty n_unique ## age 0 502 502 3 8 0 4 ## DistributionChannel 0 502 502 4 4 0 1 ## email 0 502 502 9 21 0 404 ## first_name 0 502 502 2 9 0 379 ## gender 0 502 502 4 17 0 4 ## job 0 502 502 4 49 0 351 ## name 0 502 502 7 28 0 502 ## phone_number 0 502 502 11 20 0 502 ## Q1 0 502 502 5 14 0 6 ## Q2 0 502 502 18 34 0 5 ## Q3_10 184 318 502 5 5 0 1 ## Q3_10_TEXT 184 318 502 51 135 0 318 ## Q3_4 201 301 502 26 26 0 1 ## Q3_5 165 337 502 22 22 0 1 ## Q3_6 174 328 502 21 21 0 1 ## Q3_7 172 330 502 19 19 0 1 ## Q3_8 184 318 502 18 18 0 1 ## Q3_9 162 340 502 23 23 0 1 ## Q4 0 502 502 11 22 0 7 ## Q5 0 502 502 53 134 0 502 ## ResponseId 0 502 502 17 17 0 502 ## Status 0 502 502 11 11 0 1 ## ## ── Variable type:logical ────────────────────────────────────────────────── ## variable missing complete n mean count ## ExternalReference 502 0 502 NaN 502 ## Finished 0 502 502 1 TRU: 502, NA: 0 ## IPAddress 502 0 502 NaN 502 ## RecipientEmail 502 0 502 NaN 502 ## RecipientFirstName 502 0 502 NaN 502 ## RecipientLastName 502 0 502 NaN 502 ## UserLanguage 502 0 502 NaN 502 ## ## ── Variable type:numeric ────────────────────────────────────────────────── ## variable missing complete n mean sd p0 p25 ## Duration (in seconds) 0 502 502 0.024 0.15 0 0 ## LocationLatitude 0 502 502 37.77 0 37.77 37.77 ## LocationLongitude 0 502 502 -122.41 0 -122.41 -122.41 ## Progress 0 502 502 100 0 100 100 ## p50 p75 p100 hist ## 0 0 1 ▇▁▁▁▁▁▁▁ ## 37.77 37.77 37.77 ▁▁▁▇▁▁▁▁ ## -122.41 -122.41 -122.41 ▁▁▁▇▁▁▁▁ ## 100 100 100 ▁▁▁▇▁▁▁▁ ## ## ── Variable type:POSIXct ────────────────────────────────────────────────── ## variable missing complete n min max median ## EndDate 0 502 502 2019-01-15 2019-01-15 2019-01-15 ## RecordedDate 0 502 502 2019-01-15 2019-01-15 2019-01-15 ## StartDate 0 502 502 2019-01-15 2019-01-15 2019-01-15 ## n_unique ## 74 ## 74 ## 74 3.2.5 Removing duplicate values Respondents may come back to the survey, or try to take the survey a second time on a new device. To ensure a respondent isn’t counted more than once in a survey, be sure to check for duplicate values by using a unique identifier. Common unique indentifiers include: email, embedded user id, or IP address. View duplicates using janitor package library(janitor) df %&gt;% get_dupes(email) # get_dupes is a function available through janitor, can use more than one column to view duplicates ## # A tibble: 120 x 37 ## email dupe_count StartDate EndDate Status ## &lt;S3:&gt; &lt;int&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;chr&gt; ## 1 dr@g… 10 2019-01-15 13:36:52 2019-01-15 13:36:52 Surve… ## 2 dr@g… 10 2019-01-15 13:36:53 2019-01-15 13:36:53 Surve… ## 3 dr@g… 10 2019-01-15 13:36:54 2019-01-15 13:36:54 Surve… ## 4 dr@g… 10 2019-01-15 13:37:06 2019-01-15 13:37:06 Surve… ## 5 dr@g… 10 2019-01-15 13:37:23 2019-01-15 13:37:23 Surve… ## 6 dr@g… 10 2019-01-15 13:37:26 2019-01-15 13:37:26 Surve… ## 7 dr@g… 10 2019-01-15 13:37:28 2019-01-15 13:37:28 Surve… ## 8 dr@g… 10 2019-01-15 13:37:29 2019-01-15 13:37:29 Surve… ## 9 dr@g… 10 2019-01-15 13:37:34 2019-01-15 13:37:34 Surve… ## 10 dr@g… 10 2019-01-15 13:37:58 2019-01-15 13:37:58 Surve… ## # … with 110 more rows, and 32 more variables: IPAddress &lt;lgl&gt;, ## # Progress &lt;dbl&gt;, `Duration (in seconds)` &lt;dbl&gt;, Finished &lt;lgl&gt;, ## # RecordedDate &lt;dttm&gt;, ResponseId &lt;chr&gt;, RecipientLastName &lt;lgl&gt;, ## # RecipientFirstName &lt;lgl&gt;, RecipientEmail &lt;lgl&gt;, ## # ExternalReference &lt;lgl&gt;, LocationLatitude &lt;dbl&gt;, ## # LocationLongitude &lt;dbl&gt;, DistributionChannel &lt;chr&gt;, ## # UserLanguage &lt;lgl&gt;, Q1 &lt;chr&gt;, Q2 &lt;chr&gt;, Q3_4 &lt;chr&gt;, Q3_5 &lt;chr&gt;, ## # Q3_6 &lt;chr&gt;, Q3_7 &lt;chr&gt;, Q3_8 &lt;chr&gt;, Q3_9 &lt;chr&gt;, Q3_10 &lt;chr&gt;, ## # Q3_10_TEXT &lt;chr&gt;, Q4 &lt;chr&gt;, Q5 &lt;chr&gt;, name &lt;chr&gt;, first_name &lt;chr&gt;, ## # job &lt;chr&gt;, phone_number &lt;chr&gt;, gender &lt;chr&gt;, age &lt;chr&gt; Manual way to view duplicates u_id &lt;- quo(email) # Store unique identifier column, can be IP address, email, etc. df %&gt;% group_by(!!u_id) %&gt;% tally() %&gt;% filter(n &gt; 1) ## # A tibble: 22 x 2 ## email n ## &lt;S3: glue&gt; &lt;int&gt; ## 1 dr@gmail.com 10 ## 2 dr@hotmail.com 13 ## 3 dr@me.com 10 ## 4 dr@outlook.com 8 ## 5 dr@yahoo.com 7 ## 6 gannon@me.com 2 ## 7 miss@hotmail.com 2 ## 8 miss@outlook.com 5 ## 9 miss@yahoo.com 6 ## 10 mr@gmail.com 5 ## # … with 12 more rows You’ll want to remove duplicate responses, and keep the most recent response. library(lubridate) # load library for converting datetimes #Remove duplicate emails, keep most recent submission df &lt;- df %&gt;% mutate(EndDate = as_datetime(EndDate, tz = &quot;America/Los_Angeles&quot;)) %&gt;% # converts column to a datetime format filter(!is.na(!!u_id)) %&gt;% group_by(!!u_id) %&gt;% slice(which.max(EndDate)) %&gt;% ungroup() References "],
["appendixA.html", "A Setting up R A.1 Package installation", " A Setting up R A.1 Package installation You’ll want to install the following packages: library(tidyverse) "],
["appendixB.html", "B Setting up python", " B Setting up python # Pandas makes working with data tables easier import pandas as pd # Numpy is a library for working with Arrays import numpy as np # Module for plotting graphs import matplotlib.pyplot as plt import seaborn as sns # SciPy implements many different numerical algorithms import scipy.stats as stats import collections "],
["appendixC.html", "C Generating fake data", " C Generating fake data Here’s the code I used to create the respondent information table library(charlatan) # library of fake data library(glue) # library for pasting together variables email_domains &lt;- c(&quot;@gmail.com&quot;, &quot;@hotmail.com&quot;, &quot;@outlook.com&quot;, &quot;@me.com&quot;, &quot;@yahoo.com&quot;) respondent_data &lt;- ch_generate(&#39;name&#39;, &#39;job&#39;, &#39;phone_number&#39;, n = nrow(df)) %&gt;% separate(name, &quot;first_name&quot;, extra = &quot;drop&quot;, remove=FALSE) %&gt;% mutate(email = glue(&quot;{first_lower}{email_domain}&quot;, first_lower = tolower(first_name), email_domain = sample(email_domains, nrow(df), replace=TRUE) ), gender = sample(c(&quot;male&quot;, &quot;female&quot;, &quot;other&quot;, &quot;prefer not to say&quot;), nrow(df), replace=TRUE, prob=c(0.55, 0.35, 0.05, 0.05)), age = sample(c(&quot;Under 18&quot;, &quot;18-34&quot;, &quot;35-54&quot;, &quot;55+&quot;), nrow(df), replace=TRUE, prob = c(0.05, 0.25, 0.40, 0.30)) ) # add ResponseId column from survey sample respondent_data &lt;- df %&gt;% select(ResponseId) %&gt;% bind_cols(respondent_data) write_csv(respondent_data, &quot;./sample_data/respondent_data.csv&quot;) # Store data in sample_data folder skim(respondent_data) ## Skim summary statistics ## n obs: 502 ## n variables: 8 ## ## ── Variable type:character ──────────────────────────────────────────────── ## variable missing complete n min max empty n_unique ## age 0 502 502 3 8 0 4 ## email 0 502 502 9 21 0 404 ## first_name 0 502 502 2 9 0 379 ## gender 0 502 502 4 17 0 4 ## job 0 502 502 4 49 0 351 ## name 0 502 502 7 28 0 502 ## phone_number 0 502 502 11 20 0 502 ## ResponseId 0 502 502 17 17 0 502 "],
["references.html", "References", " References "]
]
