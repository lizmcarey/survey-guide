\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\PassOptionsToPackage{usenames,dvipsnames}{color} % color is loaded by hyperref
\hypersetup{unicode=true,
            pdftitle={A Quick Start Guide to Survey Research},
            pdfauthor={Liz Carey (and hopefully many others)},
            colorlinks=true,
            linkcolor=Maroon,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{A Quick Start Guide to Survey Research}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Liz Carey (and hopefully many others)}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{2019-01-25}

\usepackage{booktabs}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=black}
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter*{Welcome to survey research}\label{welcome-to-survey-research}
\addcontentsline{toc}{chapter}{Welcome to survey research}

\begin{figure}
\centering
\includegraphics{figs/sponge_bob.png}
\caption{}
\end{figure}

This book is intended to be a quick resource for conducting survey
research. By no means is it intended to be comprehensive of all survey
research methodologies.

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

Hopefully you'll find this book to be a condensed and easy to read
resource on survey research.

We developed this book in the hopes of future collaboration among other
UX researchers.

\section*{Outline}\label{outline}
\addcontentsline{toc}{section}{Outline}

The content of the book will include:

\begin{itemize}
\tightlist
\item
  \textbf{Chapter 1}
\item
  \textbf{Chapter 2}
\end{itemize}

\section*{Prerequisites}\label{prerequisites}
\addcontentsline{toc}{section}{Prerequisites}

All you need is an interest in conducting survey research and basic data
analysis, we'll include code snippets (python and R) along the way.

\section*{Acknowledgements}\label{acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

This book wouldn't be possible without the contributions of:

\chapter{Designing a survey}\label{macro}

\section{What is your research goal?}\label{what-is-your-research-goal}

First, establish if a survey is the right method to accomplish your
research goal by asking yourself:

\begin{itemize}
\tightlist
\item
  What do you currently know?
\item
  What \emph{don't} you know?
\end{itemize}

Below is a useful visualization from the Nielsen Norman group on how to
decide between which qualitative or quantitative methods to answer your
research goal \citep{nng_method}.

\begin{center}\includegraphics{figs/nng_ux_methods_chart2} \end{center}

Surveys are great for answering the ``How many and how much'' of what
people do and say; surveys are not the best method at understanding the
``Why and how to fix'' a product problem.

\begin{figure}
\centering
\includegraphics{figs/nng_ux_methods_chart.png}
\caption{}
\end{figure}

\section{Who are you studying?}\label{who-are-you-studying}

This question may be simple at first, but when you start to narrow down

\chapter{Writing effective survey
questions}\label{writing-effective-survey-questions}

Effective survey questions result in \textbf{consistent} and
\textbf{reliable} responses.

Surveys are NOT a shortcut for usability tests

\chapter{Survey Analysis}\label{analysis}

After you've fielded your survey, here are the steps to making sense of
the data.

This section assumes you have a laptop set up to work with in either R
or python. Head over to the Appendix page if you need help with set up.

\section{Organize your workspace}\label{organize-your-workspace}

Before beginning any analysis, you'll want to set up a reproducible
workflow. Below is an adapted suggestion on how to organize your
workspace from Ben Marwick, Carl Boettiger, and Lincoln Mullen
\citep{reproducible_workflow}. Keeping your workspace organized is the
best way for you and others to understand and reproduce your analysis.

\begin{verbatim}
project
|- DESCRIPTION          # project metadata and dependencies 
|- README.md            # top-level description of content and guide to users
|
|- data/                # data files used 
|  +- raw_data.csv      # data files in open formats such as TXT, CSV, TSV, etc.
|  +- cleaned_data.csv  # data files that have been cleaned, merged, etc that you'll use for survey analysis
|
|- analysis/            # any programmatic code
|  +- my_report.Rmd     # R markdown file with narrative text interwoven with code chunks 
|  +- makefile          # builds a PDF/HTML/DOCX file from the Rmd, code, and data files
|  +- scripts/          # code files (R, shell, etc.) used for data cleaning, analysis and visualisation
|  +- figures/          # saved outputs of your figures
|
|- R/                     
|  +- my_functions.R    # custom R functions that are used more than once throughout the project
|
|- man/
|  +- my_functions.Rd   # documentation for the R functions (auto-generated when using devtools)
|
\end{verbatim}

\textbf{R version}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#List the directory names you want to create}
\NormalTok{folder_names <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"data"}\NormalTok{, }
                    \StringTok{"data/raw"}\NormalTok{, }
                    \StringTok{"data/clean"}\NormalTok{, }
                  \StringTok{"analysis"}\NormalTok{, }
                    \StringTok{"analysis/scripts"}\NormalTok{, }
                     \StringTok{"analysis/figures"}\NormalTok{, }
                  \StringTok{"R"}\NormalTok{)}

\CommentTok{#Create the directories}
\KeywordTok{sapply}\NormalTok{(folder_names, dir.create)}
\end{Highlighting}
\end{Shaded}

\section{Data Cleaning}\label{data-cleaning}

Before you can begin looking at the results, you'll need to clean the
data. By ``cleaning'' the data, we mean edited the raw file into a
format that will make the analysis valid and easier.

\subsection{Load the data}\label{load-the-data}

Download your raw survey data as a csv and load it into your your
analysis tool of choice (e.g.~Ipython notebook or Rstudio)

\textbf{R version}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load necessary packages for analysis}
\KeywordTok{library}\NormalTok{(tidyverse)        }\CommentTok{#contains all the library packages to manipulate and transform data}
\KeywordTok{library}\NormalTok{(summarytools)     }\CommentTok{#shortcut tools to visualize summaries of the data}

\CommentTok{# read/store the data as the variable df (short for dataframe)}
\CommentTok{# replace "file" with "https://raw.githubusercontent.com/lizmcarey/survey-guide/master/sample_data/Survey_test_data.csv" to download the survey data set}
\NormalTok{file <-}\StringTok{ "./sample_data/Survey_test_data.csv"} \CommentTok{#load file from folder heirarchy }
\NormalTok{df <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(file)}
\end{Highlighting}
\end{Shaded}

\textbf{python version}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#load necessary modules for analysis}
\NormalTok{import pandas as pd}

\CommentTok{#read/store the data as the variable df (short for dataframe)}
\NormalTok{df =}\StringTok{ }\KeywordTok{pd.read_csv}\NormalTok{(filename)}
\end{Highlighting}
\end{Shaded}

\subsection{Loading Qualtrics data}\label{loading-qualtrics-data}

When you download a csv from Qualtrics, it will come with a few extra
rows you don't need. Here are some automated scripts you can add to your
makefile to speed up your workflow

\textbf{R version manual}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Store the column names by reading in the column header}
\NormalTok{df_names <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(file, }\DataTypeTok{n_max=}\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{names}\NormalTok{()}

\CommentTok{# Read the entire file}
\NormalTok{df <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(file, }
               \DataTypeTok{col_names =}\NormalTok{ df_names, }\CommentTok{# use df_names to title the columns}
               \DataTypeTok{skip =} \DecValTok{3}\NormalTok{)             }\CommentTok{# skip the first three lines}

\CommentTok{#store the question names}
\NormalTok{question_bank <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(file, }\DataTypeTok{n_max=}\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{  }\CommentTok{# read in the first row of the file}
\StringTok{                 }\KeywordTok{select}\NormalTok{(}\KeywordTok{starts_with}\NormalTok{(}\StringTok{"Q"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# select columns that start with Q}
\StringTok{                 }\KeywordTok{gather}\NormalTok{(key, question_text)   }\CommentTok{# transform data from wide to long}
\end{Highlighting}
\end{Shaded}

\textbf{R version programmatic}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#function to load qualtrics csv and remove extra rows}
\NormalTok{load_qualtrics_csv <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(file) \{}
\NormalTok{  df_names <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(file, }\DataTypeTok{n_max =} \DecValTok{0}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{names}\NormalTok{()}
  
\NormalTok{  df <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(file, }\DataTypeTok{col_names =}\NormalTok{ df_names, }\DataTypeTok{skip =} \DecValTok{3}\NormalTok{)}
\NormalTok{\}}

\CommentTok{#function to store questions}
\NormalTok{get_questions <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(file) \{}
\NormalTok{  qb <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(file, }\DataTypeTok{n_max =} \DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{select}\NormalTok{(}\KeywordTok{starts_with}\NormalTok{(}\StringTok{"Q"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{gather}\NormalTok{(key, question_text)}
\NormalTok{\}}

\CommentTok{#Use function to read in survey file, and skip first 3 lines}
\NormalTok{df <-}\StringTok{ }\KeywordTok{load_qualtrics_csv}\NormalTok{(file)}

\CommentTok{#Use function to store question wording}
\NormalTok{question_bank <-}\StringTok{ }\KeywordTok{get_questions}\NormalTok{(file)}
\end{Highlighting}
\end{Shaded}

\subsection{Preview the data}\label{preview-the-data}

It's important to get a look at the data to spot any errors in uploading
the dataset and the validity of the responses.

You'll want to check for:

\begin{itemize}
\tightlist
\item
  Total number of observations/rows
\item
  Duplicate responses
\item
  Drop off/Abandon rate of the survey
\item
  Average survey completion time
\item
  ``Speeders:'' those who couldn't have completed the survey in a
  reasonable amount of time
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

There are multiple different ways to preview your dataset before
analysis. One quick way is to check the first few rows of your data. You
can do this with the function \texttt{head()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Check the first 5 rows of data}
\KeywordTok{head}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 29
##   StartDate           EndDate             Status      IPAddress Progress
##   <dttm>              <dttm>              <chr>       <chr>        <int>
## 1 2019-01-15 13:28:39 2019-01-15 13:28:39 Survey Test <NA>           100
## 2 2019-01-15 13:28:40 2019-01-15 13:28:40 Survey Test <NA>           100
## 3 2019-01-15 13:36:47 2019-01-15 13:36:47 Survey Test <NA>           100
## 4 2019-01-15 13:36:47 2019-01-15 13:36:47 Survey Test <NA>           100
## 5 2019-01-15 13:36:48 2019-01-15 13:36:48 Survey Test <NA>           100
## 6 2019-01-15 13:36:48 2019-01-15 13:36:48 Survey Test <NA>           100
## # ... with 24 more variables: `Duration (in seconds)` <int>,
## #   Finished <chr>, RecordedDate <dttm>, ResponseId <chr>,
## #   RecipientLastName <chr>, RecipientFirstName <chr>,
## #   RecipientEmail <chr>, ExternalReference <chr>, LocationLatitude <dbl>,
## #   LocationLongitude <dbl>, DistributionChannel <chr>,
## #   UserLanguage <chr>, Q1 <chr>, Q2 <chr>, Q3_4 <chr>, Q3_5 <chr>,
## #   Q3_6 <chr>, Q3_7 <chr>, Q3_8 <chr>, Q3_9 <chr>, Q3_10 <chr>,
## #   Q3_10_TEXT <chr>, Q4 <chr>, Q5 <chr>
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

A more comprehensive way to view your dataset is with the \texttt{skimr}
package. This package will give an overview of the number of
observations and variables in your data.

The missing column should not be greater than 20\% of your total number
of observations (unless it's a multiselect question).

Questions with dropoff greater than 20\% can signal that the question
was difficult for respondents to answer; you should be wary of response
bias and consider removing the question from analysis and rewording the
question for future survey sends.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(skimr)}
\KeywordTok{skim}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Skim summary statistics
##  n obs: 502 
##  n variables: 29 
## 
## -- Variable type:character ------------------------------------------------
##             variable missing complete   n min  max empty n_unique
##  DistributionChannel       0      502 502   4    4     0        1
##    ExternalReference     502        0 502 Inf -Inf     0        0
##             Finished       0      502 502   4    4     0        1
##            IPAddress     502        0 502 Inf -Inf     0        0
##                   Q1       0      502 502   5   14     0        6
##                   Q2       0      502 502  18   34     0        5
##                Q3_10     184      318 502   5    5     0        1
##           Q3_10_TEXT     184      318 502  51  135     0      318
##                 Q3_4     201      301 502  26   26     0        1
##                 Q3_5     165      337 502  22   22     0        1
##                 Q3_6     174      328 502  21   21     0        1
##                 Q3_7     172      330 502  19   19     0        1
##                 Q3_8     184      318 502  18   18     0        1
##                 Q3_9     162      340 502  23   23     0        1
##                   Q4       0      502 502  11   22     0        7
##                   Q5       0      502 502  53  134     0      502
##       RecipientEmail     502        0 502 Inf -Inf     0        0
##   RecipientFirstName     502        0 502 Inf -Inf     0        0
##    RecipientLastName     502        0 502 Inf -Inf     0        0
##           ResponseId       0      502 502  17   17     0      502
##               Status       0      502 502  11   11     0        1
##         UserLanguage     502        0 502 Inf -Inf     0        0
## 
## -- Variable type:integer --------------------------------------------------
##               variable missing complete   n    mean   sd  p0 p25 p50 p75
##  Duration (in seconds)       0      502 502   0.024 0.15   0   0   0   0
##               Progress       0      502 502 100     0    100 100 100 100
##  p100     hist
##     1 ▇▁▁▁▁▁▁▁
##   100 ▁▁▁▇▁▁▁▁
## 
## -- Variable type:numeric --------------------------------------------------
##           variable missing complete   n    mean sd      p0     p25     p50
##   LocationLatitude       0      502 502   37.77  0   37.77   37.77   37.77
##  LocationLongitude       0      502 502 -122.41  0 -122.41 -122.41 -122.41
##      p75    p100     hist
##    37.77   37.77 ▁▁▁▇▁▁▁▁
##  -122.41 -122.41 ▁▁▁▇▁▁▁▁
## 
## -- Variable type:POSIXct --------------------------------------------------
##      variable missing complete   n        min        max     median
##       EndDate       0      502 502 2019-01-15 2019-01-15 2019-01-15
##  RecordedDate       0      502 502 2019-01-15 2019-01-15 2019-01-15
##     StartDate       0      502 502 2019-01-15 2019-01-15 2019-01-15
##  n_unique
##        74
##        74
##        74
\end{verbatim}

Another package that can give a brief overview of your data is
\texttt{summarytools}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(summarytools)}
\KeywordTok{view}\NormalTok{(}\KeywordTok{dfSummary}\NormalTok{(df)) }\CommentTok{# use view lowercase to see html output in the Rstudio viewer pane}
\end{Highlighting}
\end{Shaded}

\subsection{Joining data sets}\label{joining-data-sets}

Sometimes the data you need lives in two tables. \texttt{dplyr} from the
\texttt{tidyverse} package makes it easy to join your data sets
together. In order to join two tables together, you'll need a shared
unique identifier across the two tables.

Below are all the ways you can join two data sets using R and the
corresponding \texttt{dplyr} functions.

\begin{figure}
\centering
\includegraphics{figs/combine_datasets.png}
\caption{}
\end{figure}

You can view this image and additional ways to transform data sets on
the
\href{https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf}{RStudio
Data Wrangling Cheat Sheet}.

In \protect\hyperlink{appendixC}{Appendix C}, I've generated a fake
dataset with corresponding ResponseId's that match to the survey data
set (df).

Below I use a left join to merge respondent data table with the survey
data table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\NormalTok{df }\OperatorTok{%>%}\StringTok{ }\KeywordTok{left_join}\NormalTok{(respondent_data, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"ResponseId"}\NormalTok{ =}\StringTok{ "ResponseId"}\NormalTok{))}

\CommentTok{# View merged data sets}
\KeywordTok{skim}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Skim summary statistics
##  n obs: 502 
##  n variables: 36 
## 
## -- Variable type:character ------------------------------------------------
##             variable missing complete   n min  max empty n_unique
##                  age       0      502 502   3    8     0        4
##  DistributionChannel       0      502 502   4    4     0        1
##                email       0      502 502   9   21     0      409
##    ExternalReference     502        0 502 Inf -Inf     0        0
##             Finished       0      502 502   4    4     0        1
##           first_name       0      502 502   2   11     0      378
##               gender       0      502 502   4   17     0        4
##            IPAddress     502        0 502 Inf -Inf     0        0
##                  job       0      502 502   3   59     0      350
##                 name       0      502 502   9   28     0      502
##         phone_number       0      502 502  11   20     0      502
##                   Q1       0      502 502   5   14     0        6
##                   Q2       0      502 502  18   34     0        5
##                Q3_10     184      318 502   5    5     0        1
##           Q3_10_TEXT     184      318 502  51  135     0      318
##                 Q3_4     201      301 502  26   26     0        1
##                 Q3_5     165      337 502  22   22     0        1
##                 Q3_6     174      328 502  21   21     0        1
##                 Q3_7     172      330 502  19   19     0        1
##                 Q3_8     184      318 502  18   18     0        1
##                 Q3_9     162      340 502  23   23     0        1
##                   Q4       0      502 502  11   22     0        7
##                   Q5       0      502 502  53  134     0      502
##       RecipientEmail     502        0 502 Inf -Inf     0        0
##   RecipientFirstName     502        0 502 Inf -Inf     0        0
##    RecipientLastName     502        0 502 Inf -Inf     0        0
##           ResponseId       0      502 502  17   17     0      502
##               Status       0      502 502  11   11     0        1
##         UserLanguage     502        0 502 Inf -Inf     0        0
## 
## -- Variable type:integer --------------------------------------------------
##               variable missing complete   n    mean   sd  p0 p25 p50 p75
##  Duration (in seconds)       0      502 502   0.024 0.15   0   0   0   0
##               Progress       0      502 502 100     0    100 100 100 100
##  p100     hist
##     1 ▇▁▁▁▁▁▁▁
##   100 ▁▁▁▇▁▁▁▁
## 
## -- Variable type:numeric --------------------------------------------------
##           variable missing complete   n    mean sd      p0     p25     p50
##   LocationLatitude       0      502 502   37.77  0   37.77   37.77   37.77
##  LocationLongitude       0      502 502 -122.41  0 -122.41 -122.41 -122.41
##      p75    p100     hist
##    37.77   37.77 ▁▁▁▇▁▁▁▁
##  -122.41 -122.41 ▁▁▁▇▁▁▁▁
## 
## -- Variable type:POSIXct --------------------------------------------------
##      variable missing complete   n        min        max     median
##       EndDate       0      502 502 2019-01-15 2019-01-15 2019-01-15
##  RecordedDate       0      502 502 2019-01-15 2019-01-15 2019-01-15
##     StartDate       0      502 502 2019-01-15 2019-01-15 2019-01-15
##  n_unique
##        74
##        74
##        74
\end{verbatim}

\subsection{Removing duplicate values}\label{removing-duplicate-values}

Respondents may come back to the survey, or try to take the survey a
second time on a new device. To ensure a respondent isn't counted more
than once in a survey, be sure to check for duplicate values by using a
unique identifier. Common unique indentifiers include: email, embedded
user id, or IP address.

\textbf{View duplicates using janitor package}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(janitor)}

\NormalTok{df }\OperatorTok{%>%}\StringTok{ }\KeywordTok{get_dupes}\NormalTok{(email) }\CommentTok{# get_dupes is a function available through janitor, can use more than one column to view duplicates}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 117 x 37
##    email       dupe_count StartDate           EndDate             Status  
##    <S3: glue>       <int> <dttm>              <dttm>              <chr>   
##  1 ammon@gmai~          2 2019-01-15 13:37:01 2019-01-15 13:37:01 Survey ~
##  2 ammon@gmai~          2 2019-01-15 13:37:34 2019-01-15 13:37:34 Survey ~
##  3 dr@gmail.c~         12 2019-01-15 13:36:59 2019-01-15 13:36:59 Survey ~
##  4 dr@gmail.c~         12 2019-01-15 13:37:01 2019-01-15 13:37:01 Survey ~
##  5 dr@gmail.c~         12 2019-01-15 13:37:08 2019-01-15 13:37:08 Survey ~
##  6 dr@gmail.c~         12 2019-01-15 13:37:16 2019-01-15 13:37:16 Survey ~
##  7 dr@gmail.c~         12 2019-01-15 13:37:20 2019-01-15 13:37:20 Survey ~
##  8 dr@gmail.c~         12 2019-01-15 13:37:20 2019-01-15 13:37:20 Survey ~
##  9 dr@gmail.c~         12 2019-01-15 13:37:25 2019-01-15 13:37:25 Survey ~
## 10 dr@gmail.c~         12 2019-01-15 13:37:35 2019-01-15 13:37:35 Survey ~
## # ... with 107 more rows, and 32 more variables: IPAddress <chr>,
## #   Progress <int>, `Duration (in seconds)` <int>, Finished <chr>,
## #   RecordedDate <dttm>, ResponseId <chr>, RecipientLastName <chr>,
## #   RecipientFirstName <chr>, RecipientEmail <chr>,
## #   ExternalReference <chr>, LocationLatitude <dbl>,
## #   LocationLongitude <dbl>, DistributionChannel <chr>,
## #   UserLanguage <chr>, Q1 <chr>, Q2 <chr>, Q3_4 <chr>, Q3_5 <chr>,
## #   Q3_6 <chr>, Q3_7 <chr>, Q3_8 <chr>, Q3_9 <chr>, Q3_10 <chr>,
## #   Q3_10_TEXT <chr>, Q4 <chr>, Q5 <chr>, name <chr>, first_name <chr>,
## #   job <chr>, phone_number <chr>, gender <chr>, age <chr>
\end{verbatim}

\textbf{Manual way to view duplicates}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{u_id <-}\StringTok{ }\KeywordTok{quo}\NormalTok{(email) }\CommentTok{# Store unique identifier column, can be IP address, email, etc. }

\NormalTok{df }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(}\OperatorTok{!!}\NormalTok{u_id) }\OperatorTok{%>%}\StringTok{ }
\StringTok{       }\KeywordTok{tally}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{       }\KeywordTok{filter}\NormalTok{(n }\OperatorTok{>}\StringTok{ }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 24 x 2
##    email                 n
##    <S3: glue>        <int>
##  1 ammon@gmail.com       2
##  2 dr@gmail.com         12
##  3 dr@hotmail.com        6
##  4 dr@me.com             9
##  5 dr@outlook.com        7
##  6 dr@yahoo.com         13
##  7 madilyn@gmail.com     2
##  8 miss@gmail.com        5
##  9 miss@hotmail.com      4
## 10 mr@gmail.com          6
## # ... with 14 more rows
\end{verbatim}

You'll want to remove duplicate responses, and keep the most recent
response.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(lubridate) }\CommentTok{# load library for converting datetimes}

\CommentTok{#Remove duplicate emails, keep most recent submission}
\NormalTok{df <-}\StringTok{ }\NormalTok{df }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{EndDate =} \KeywordTok{as_datetime}\NormalTok{(EndDate, }\DataTypeTok{tz =} \StringTok{"America/Los_Angeles"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# converts column to a datetime format }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(}\OperatorTok{!!}\NormalTok{u_id)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(}\OperatorTok{!!}\NormalTok{u_id) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\KeywordTok{which.max}\NormalTok{(EndDate)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ungroup}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\appendix


\chapter{Setting up R}\label{appendixA}

\section{Package installation}\label{package-installation}

You'll want to install the following packages:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\chapter{Setting up python}\label{appendixB}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Pandas makes working with data tables easier}
\NormalTok{import pandas as pd}

\CommentTok{# Numpy is a library for working with Arrays}
\NormalTok{import numpy as np}

\CommentTok{# Module for plotting graphs}
\NormalTok{import matplotlib.pyplot as plt}
\NormalTok{import seaborn as sns}

\CommentTok{# SciPy implements many different numerical algorithms}
\NormalTok{import scipy.stats as stats}
\NormalTok{import collections}
\end{Highlighting}
\end{Shaded}

\hypertarget{appendixC}{\chapter{Generating fake data}\label{appendixC}}

Here's the code I used to create the respondent information table

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(charlatan) }\CommentTok{# library of fake data}
\KeywordTok{library}\NormalTok{(glue) }\CommentTok{# library for pasting together variables}

\NormalTok{email_domains <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"@gmail.com"}\NormalTok{, }\StringTok{"@hotmail.com"}\NormalTok{, }\StringTok{"@outlook.com"}\NormalTok{, }\StringTok{"@me.com"}\NormalTok{, }\StringTok{"@yahoo.com"}\NormalTok{)}

\NormalTok{respondent_data <-}\StringTok{ }\KeywordTok{ch_generate}\NormalTok{(}\StringTok{'name'}\NormalTok{, }\StringTok{'job'}\NormalTok{, }\StringTok{'phone_number'}\NormalTok{, }\DataTypeTok{n =} \KeywordTok{nrow}\NormalTok{(df)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{                    }\KeywordTok{separate}\NormalTok{(name, }\StringTok{"first_name"}\NormalTok{, }\DataTypeTok{extra =} \StringTok{"drop"}\NormalTok{, }\DataTypeTok{remove=}\OtherTok{FALSE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{                    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{email =} \KeywordTok{glue}\NormalTok{(}\StringTok{"\{first_lower\}\{email_domain\}"}\NormalTok{, }
                                          \DataTypeTok{first_lower =} \KeywordTok{tolower}\NormalTok{(first_name), }
                                          \DataTypeTok{email_domain =} \KeywordTok{sample}\NormalTok{(email_domains, }\KeywordTok{nrow}\NormalTok{(df), }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{) ), }
                           \DataTypeTok{gender =} \KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"male"}\NormalTok{, }\StringTok{"female"}\NormalTok{, }\StringTok{"other"}\NormalTok{, }\StringTok{"prefer not to say"}\NormalTok{), }\KeywordTok{nrow}\NormalTok{(df), }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{prob=}\KeywordTok{c}\NormalTok{(}\FloatTok{0.55}\NormalTok{, }\FloatTok{0.35}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.05}\NormalTok{)), }
                           \DataTypeTok{age =} \KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"Under 18"}\NormalTok{, }\StringTok{"18-34"}\NormalTok{, }\StringTok{"35-54"}\NormalTok{, }\StringTok{"55+"}\NormalTok{), }\KeywordTok{nrow}\NormalTok{(df), }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{0.40}\NormalTok{, }\FloatTok{0.30}\NormalTok{))}
\NormalTok{        )}

\CommentTok{# add ResponseId column from survey sample}
\NormalTok{respondent_data <-}\StringTok{ }\NormalTok{df }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(ResponseId) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{bind_cols}\NormalTok{(respondent_data)}

\KeywordTok{write_csv}\NormalTok{(respondent_data, }\StringTok{"./sample_data/respondent_data.csv"}\NormalTok{) }\CommentTok{# Store data in sample_data folder}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{skim}\NormalTok{(respondent_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Skim summary statistics
##  n obs: 502 
##  n variables: 8 
## 
## -- Variable type:character ------------------------------------------------
##      variable missing complete   n min max empty n_unique
##           age       0      502 502   3   8     0        4
##         email       0      502 502   9  21     0      409
##    first_name       0      502 502   2  11     0      378
##        gender       0      502 502   4  17     0        4
##           job       0      502 502   3  59     0      350
##          name       0      502 502   9  28     0      502
##  phone_number       0      502 502  11  20     0      502
##    ResponseId       0      502 502  17  17     0      502
\end{verbatim}

\bibliography{book.bib,packages.bib}


\end{document}
